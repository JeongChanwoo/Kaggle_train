{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import io\n",
    "\n",
    "#파일 압축 용도\n",
    "import gzip\n",
    "import pickle\n",
    "import zlib\n",
    "\n",
    "# 데이터, 배열\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 범주형 수치형 변환\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "np.random.seed(2016)\n",
    "transformers={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = (\n",
    "    \"ind_ahor_fin_ult1\",\n",
    "    \"ind_aval_fin_ult1\",\n",
    "    \"ind_cco_fin_ult1\" ,\n",
    "    \"ind_cder_fin_ult1\",\n",
    "    \"ind_cno_fin_ult1\" ,\n",
    "    \"ind_ctju_fin_ult1\",\n",
    "    \"ind_ctma_fin_ult1\",\n",
    "    \"ind_ctop_fin_ult1\",\n",
    "    \"ind_ctpp_fin_ult1\",\n",
    "    \"ind_deco_fin_ult1\",\n",
    "    \"ind_deme_fin_ult1\",\n",
    "    \"ind_dela_fin_ult1\",\n",
    "    \"ind_ecue_fin_ult1\",\n",
    "    \"ind_fond_fin_ult1\",\n",
    "    \"ind_hip_fin_ult1\" ,\n",
    "    \"ind_plan_fin_ult1\",\n",
    "    \"ind_pres_fin_ult1\",\n",
    "    \"ind_reca_fin_ult1\",\n",
    "    \"ind_tjcr_fin_ult1\",\n",
    "    \"ind_valo_fin_ult1\",\n",
    "    \"ind_viv_fin_ult1\" ,\n",
    "    \"ind_nomina_ult1\"  ,\n",
    "    \"ind_nom_pens_ult1\",\n",
    "    \"ind_recibo_ult1\"  ,\n",
    ")\n",
    "\n",
    "dtypes = {\n",
    "    \"fecha_dato\": str,\n",
    "    \"ncodpers\": int,\n",
    "    \"conyuemp\": str, # Spouse index. 1 if the customer is spouse of an employee\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코더 함수\n",
    "def label_encode(df, features, name):\n",
    "    # 데이터 프레임 df의 변수 name값을 모두 string으로 변환\n",
    "    df[name] = df[name].astype('str')\n",
    "    \n",
    "    #이미 라벨 인코더 했던 변수는 trasformer[name]에 있는 라벨 인코더를 재활용\n",
    "    if name in transformers:\n",
    "        df[name] = transformers[name].transform(df[name])\n",
    "        \n",
    "    #처음 나오는 변수는 transformer에 라벨인코더를 저장하고 fit_transfrom으로 라벨인코딩\n",
    "    else:\n",
    "        transformers[name] = LabelEncoder()\n",
    "        df[name] = transformers[name].fit_transform(df[name])\n",
    "        # 라벨인코딩한 변수는 features 리스트에 추가\n",
    "        features.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자체 구현 one hot encoder\n",
    "def custom_one_hot(df, features, name, names, dtype = np.int8, check = False):\n",
    "    for n, val in names.items():\n",
    "        # 신규 변수명을 \"변수명_숫자\" 지정\n",
    "        new_name = \"%s_%s\" % (name, n)\n",
    "        #기존 변수에서 해당고유값 있으면 1 그외는 0 이진변수 생성\n",
    "        df[new_name] = df[name].map(lambda x : 1 if x == val else 0).astype(dtype)\n",
    "        features.append(new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply_transform\n",
    "* 결측값 대체 : 결측 값을 0.0 or 1.0으로 대체\n",
    "* 범주형 데이터 라벨 인코딩\n",
    "* 고빈도 top 100 개를 빈도 순위로 변환 : 고빈도 데이터에 대한 선형관계 추출\n",
    "* 수치형 변수 log 변환\n",
    "* 날짜 데이터 년/월 추출\n",
    "* 날짜간 차이값을 통한 파생변수 : 두 데이터간 날짜값의 차이를 통한 상대적인 거리변수를 생성\n",
    "* 원핫 인코딩 : 범주형 데이터의 표현력을 높이기 위해 오든 고유값을 새로운 이진 변수로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨인코더, 원핫인코더, 유틸.py의 빈도 추출, 날짜 변환을 이용해 \n",
    "# 변수에 대한 전처리와 피쳐 엔지니어링 수행\n",
    "def apply_transforms(train_df):\n",
    "    \n",
    "    # 학습에 사용할 변수를 저장할 features 리스트 생성\n",
    "    features = []\n",
    "    \n",
    "    # 두 변수를 label_encode()\n",
    "    label_encode(train_df, features, \"canal_entrada\")\n",
    "    label_encode(train_df, features, \"pais_residencia\")\n",
    "    \n",
    "    # age의 결측값을 0.0으로 대체하고, 모든 값을 정수로 변환.(내 생각 - renta는 값의 범위가 크기에 log를 위해 1로 채워넣음)\n",
    "    train_df[\"age\"] = train_df[\"age\"].fillna(0.0).astype(np.int16)\n",
    "    features.append(\"age\")\n",
    "    \n",
    "    # renta 결측값을 1.0으로 대체 하고 log를 씌워 정수 변환\n",
    "    train_df[\"renta\"].fillna(1.0, inplace=True)\n",
    "    train_df[\"renta\"] = train_df[\"renta\"].map(math.log)\n",
    "    features.append(\"renta\")\n",
    "    \n",
    "    # 고빈도 100개의 순위 추출\n",
    "    train_df[\"renta_top\"] = encode_top(train_df[\"renta\"])\n",
    "    features.append(\"renta_top\")\n",
    "    \n",
    "    #결측값, 음수를 0으로 대체, 나머지는 +1.0후 정수 변환\n",
    "    train_df[\"antiguedad\"] = train_df[\"antiguedad\"].map(lambda x : 0.0 if x < 0 or math.isnan(x) else  x + 1.0).astype(np.int16)\n",
    "    features.append(\"antiguedad\")\n",
    "    \n",
    "    # 결측값을 0.0으로 대체하고, 정수로 변환\n",
    "    train_df[\"tipodom\"] = train_df[\"tipodom\"].fillna(0.0).astype(np.int8)\n",
    "    features.append(\"tipodom\")\n",
    "    \n",
    "    train_df[\"cod_prov\"] = train_df[\"cod_prov\"].fillna(0.0).astype(np.int8)\n",
    "    features.append(\"cod_prov\")\n",
    "    \n",
    "    # fecha_dato에서 월/년도 추출하여 정수값으로 변환\n",
    "    train_df[\"fecha_dato_month\"] = train_df[\"fecha_dato\"].map(lambda x : int(x.split(\"-\")[1])).astype(np.int8)\n",
    "    features.append(\"fecha_dato_month\")\n",
    "    train_df[\"fecha_dato_year\"] = train_df[\"fecha_dato\"].map(lambda x : int(x.split(\"-\")[0])).astype(np.int16)\n",
    "    features.append(\"fecha_dato_year\")\n",
    "    \n",
    "    \n",
    "    # 결측값을 0.0으로 대체하고, fecha_alta에서 월/년도를 추출하여 정수값으로 변환\n",
    "    # x.__class__를 통해 결측값 탐지.  x.__class__는 결측값일 경우 float을 반환\n",
    "    train_df[\"fecha_alta_month\"] = train_df[\"fecha_alta\"].map(lambda x : 0.0 if x.__class__ is float else float(x.split(\"-\")[1])).astype(np.int8)\n",
    "    features.append(\"fecha_alta_month\")\n",
    "    train_df[\"fecha_alta_year\"] = train_df[\"fecha_alta\"].map(lambda x : 0.0 if x.__class__ is float else float(x.split(\"-\")[0])).astype(np.int8)\n",
    "    features.append(\"fecha_alta_year\")\n",
    "    \n",
    "    #날짜 데이터를 월 기준 수치형 변수로 변환\n",
    "    train_df[\"fecha_dato_float\"] = train_df[\"fecha_dato\"].map(date_to_float)\n",
    "    train_df[\"fecha_alta_float\"] = train_df[\"fecha_dato\"].map(date_to_float)\n",
    "    \n",
    "    # fecha_dato와 fecha_alto의 월 기준 수치형 변수의 차이값을 파생 변수로 생성\n",
    "    train_df[\"dato_minus_alta\"] = train_df[\"fecha_dato_float\"] - train_df[\"fecha_alta_float\"]\n",
    "    features.append(\"dato_minus_alta\")\n",
    "    \n",
    "    # 날짜 데이터를 월 기준 수치형 변수로 변환 (1 ~ 18 사이 값으로 제한)\n",
    "    train_df[\"int_date\"] = train_df[\"fecha_dato\"].map(date_to_int).astype(np.int8)\n",
    "    \n",
    "    # 원핫 인코딩 수행\n",
    "    custom_one_hot(train_df, features, \"indresi\", {\"n\" : \"N\"})\n",
    "    custom_one_hot(train_df, features, \"indext\", {\"s\" : \"S\"})\n",
    "    custom_one_hot(train_df, features, \"conyuemp\", {\"n\" : \"N\"})\n",
    "    custom_one_hot(train_df, features, \"sexo\", {\"h\" : \"H\", \"v\":\"V\"})\n",
    "    custom_one_hot(train_df, features, \"ind_empleado\", {\"a\" : \"A\", \"b\":\"B\", \"f\": \"F\", \"n\":\"N\"})\n",
    "    custom_one_hot(train_df, features, \"ind_nuevo\", {\"new\" : \"1\"})\n",
    "    custom_one_hot(train_df, features, \"segmento\", {\"top\" : \"01 - TOP\", \"particulares\" : \"02 - PARTICULARES\", \"universitario\" : \"03 - UNIVERSITARIO\"})\n",
    "    custom_one_hot(train_df, features, \"indfall\", {\"s\" : \"S\"})\n",
    "    custom_one_hot(train_df, features, \"indrel\", {\"1\" : 1, \"99\" : 99})\n",
    "    custom_one_hot(train_df, features, \"tiprel_1mes\", {\"a\" : \"A\", \"i\":\"I\", \"p\":\"P\", \"r\":\"R\"})\n",
    "    \n",
    "    # 결측값을 0.0으로 대체 하고 그외는 +1.0 더하고 정수 변환\n",
    "    train_df[\"ind_actividad_cliente\"] = train_df[\"ind_actividad_cliente\"].map(lambda x : 0.0 if math.isnan(x) else x + 1.0).astype(np.int8)\n",
    "    features.append(\"ind_actividad_cliente\")\n",
    "    \n",
    "    \n",
    "    # 결측값을 0.0으로 대체하고, \"P\"를 5로 대체하고, 정수 변환\n",
    "    train_df[\"indrel_1mes\"] = train_df[\"indrel_1mes\"].map(lambda x : 5.0 if x ==\"P\" else x).astype(float).fillna(0.0).astype(np.int8)\n",
    "    features.append(\"indrel_1mes\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 전처리, 피처 엔지니어링이 1차적으로 완료된 데이터 프레임 train_df와 학습에 필요한 변수리스트 features를 튜플 형태로 반환\n",
    "    return train_df, tuple(features)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag 파생변수 생성\n",
    "* 시계열 문제의 고오오급 파생변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prev_df(train_df, step):\n",
    "    # 새로운 데이터 프레임에 ncodpers를 추가, int_date를 step만큼 이동시킨 값\n",
    "    prev_df = pd.DataFrame()\n",
    "    prev_df[\"ncodpers\"] = train_df[\"ncodpers\"]\n",
    "    \n",
    "    prev_df[\"int_date\"] = train_df[\"int_date\"].map(lambda x : x + step).astype(np.int8)\n",
    "    \n",
    "    # \"변수명_prev1\" 형태의 lag 변수를 생성\n",
    "    prod_features = [\"%s_prev%s\" % (prod, step) for prod in products]\n",
    "    for prod, prev in zip(products, prod_features):\n",
    "        prev_df[prev] = train_df[prod]\n",
    "        \n",
    "    return prev_df, tuple(prod_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag 변수를 훈련 데이터에 통합\n",
    "def join_with_prev(df, prev_df, how):\n",
    "    # merge를 통해 join\n",
    "    df = df.merge(prev_df, on=[\"ncodpers\", \"int_date\"], how = how)\n",
    "    # 24개 금융변수를 소수형으로 변환\n",
    "    for f in set(prev_df.columns.values.tolist()) - set([\"ncodpers\", \"int_date\"]):\n",
    "        df[f] = df[f].astype(np.float16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    fname = \"/home/jeongchanwoo/workspace/git/study/Kaggle_data/santander-product-recommendation/input/8th.clean.all.csv\"\n",
    "    train_df = pd.read_csv(fname, dtype=dtypes)\n",
    "    \n",
    "    for prod in products:\n",
    "        train_df[prod] = train_df[prod].fillna(0.0).astype(np.int8)\n",
    "        \n",
    "    # 48개 변수 피쳐 엔지니어링\n",
    "    train_df, features = apply_transforms(train_df)\n",
    "    \n",
    "    \n",
    "    # lag_5 변수 생성\n",
    "    prev_dfs = []\n",
    "    prod_features = None\n",
    "    \n",
    "    user_features = frozenset([1,2])\n",
    "    \n",
    "    # 1~5까지의 step에 대해 lag-n 데이터 생성\n",
    "    for step in range(1,6):\n",
    "        prev1_train_df, prod1_features = make_prev_df(train_df, step)\n",
    "        \n",
    "        # 생성한 lag는 prev_dfs에 저장\n",
    "        prev_dfs.append(prev1_train_df)\n",
    "        # features에 lag-1,2만 추가\n",
    "        if step in user_features:\n",
    "            features += prod1_features\n",
    "            \n",
    "        # prod_features에 lag-1 변수명만 저장\n",
    "        if step == 1:\n",
    "            prod_features = prod1_features\n",
    "            \n",
    "    return train_df, prev_dfs, features, prod_features\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    train_df, prev_dfs, features, prod_features = load_data()\n",
    "    # lag-5 변수 통합\n",
    "    for i, prev_df in enumerate(prev_dfs):\n",
    "        how = \"inner\" if i==0 else \"left\"\n",
    "        train_df = join_with_prev(train_df, prev_df, how=how)\n",
    "        \n",
    "    # 24개 금융변수에 대해 lag 별로 기초통계량을 변수화\n",
    "    for prod in products:\n",
    "        # [1~3], [1~5],[2~5] 3개 구간에 대해 표준편차\n",
    "        for begin, end in [(1,3), (1,5), (2,5)]:\n",
    "            prods = [\"%s_prev%s\" % (prod, i ) for i in range(begin - end + 1)]\n",
    "            mp_df = train_df.as_matrix(columns = prods)\n",
    "            stdf = \"%s_std_%s_%s\" % (prod, begin, end)\n",
    "            \n",
    "            # np.nanstd로 표준편차 구하고, features에 신규 파생변수 이름 추가\n",
    "            train_df[stdf] = np.nanstd(mp_df, axis=1)\n",
    "            features += (stdf,)\n",
    "            \n",
    "            \n",
    "        # [2~3], [2~5]에 대해 최솟값 최댓값 구함\n",
    "        for begin, end in [(2,3), (2,5)]:\n",
    "            prods = [\"%s_prev%s\" % (prod, i) for i in range(begin, end+1)]\n",
    "            mp_df = train_df.as_matrix(columns = prods)\n",
    "            \n",
    "            minf = \"%s_min_%s_%s\" % (prod, begin,end)\n",
    "            train_df[minf] = np.nanmin(mp_df, axis=1).astype(np.int8)\n",
    "            \n",
    "            maxf = \"%s_max_%s_%s\" % (prod, begin,end)\n",
    "            train_df[maxf] = np.nanmax(mp_df, axis=1).astype(np.int8)\n",
    "            \n",
    "            features += (minf, maxf,)\n",
    "            \n",
    "            \n",
    "    # 고객 식별 번호(ncodpers), 정수 표현 날짜(inf_date), 실제 날짜(fecha_dato), 24개 금융변수(prodcuts), 전처리/피처 엔지니어링 변수(features)가 주요변수\n",
    "    \n",
    "    leave_columns = [\"ncodpers\", \"int_date\", \"fecha_dato\"] + list(products) + list(features)\n",
    "    \n",
    "    # 중복값 확인\n",
    "    assert len(leave_columns) == len(set(leave_columns))\n",
    "    \n",
    "    # train_df에서 주요변수 추출\n",
    "    train_df = train_df[leave_columns]\n",
    "    \n",
    "    return train_df, features, prod_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도 상위 100개 데이터 순위 변수 추출\n",
    "def encode_top(s, count=100, dtype = np.int8):\n",
    "    # 고유값 빈도 계산\n",
    "    uniqs, freqs = np.unique(s, return_counts=True)\n",
    "    # 빈도 top 100 추출\n",
    "    top = sorted(zip(uniqs, freqs), key = lambda vk : vk[1], reverse= True)[:count]\n",
    "    \n",
    "    # 기존데이터 : 순위 dict 생성\n",
    "    top_map = {uf[0] : l + 1 for uf, l in zip(top, range(len(top)))}\n",
    "    \n",
    "    # 고빈도 100개의 데이터는 순위로 대체, 그외는 0으로 대체\n",
    "    return s.map(lambda x: top_map.get(x,0)).astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#날짜데이터를 월 단위 숫자로 변환 utils.py\n",
    "\n",
    "def date_to_float(str_date):\n",
    "    if str_date.__class__ is float and math.isnan(str_date) or str_date ==\"\":\n",
    "        return np.nan\n",
    "\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n",
    "    float_date = float(Y) * 12 + float(M)\n",
    "    return float_date\n",
    "\n",
    "\n",
    "#날짜데이터를 월단위로 변환하여 1~18사이로 제한\n",
    "def date_to_int(str_date):\n",
    "    Y,M,D = [int(a) for a in str_date.strip().split(\"-\")]\n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    assert 1 <= int_date <= 12+6\n",
    "    return int_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10, default=1.0):\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return default\n",
    "\n",
    "    return score / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk(actual, predicted, k=10, default=1.0):\n",
    "    return np.mean([apk(a,p,k,default) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost 모델 학습\n",
    "def xgboost(XY_train, XY_validate, test_df, features, XY_all = None, restore = False):\n",
    "    # 최적 파라메터\n",
    "    param = {\n",
    "        'objective' : 'multi:softprob',\n",
    "        'eta' : 0.1,\n",
    "        'min_child_weight' : 10,\n",
    "        'max_depth' : 8,\n",
    "        'silent' : 1,\n",
    "        'eval_metric' : 'mlogloss',\n",
    "        'colsample_bytree' : 0.8,\n",
    "        'colsample_bylevel' : 0.9,\n",
    "        'num_class' : len(products),\n",
    "        'tree_method' : 'gpu_exact',\n",
    "    }\n",
    "    \n",
    "    if not restore:\n",
    "        # 훈련 데이터에서 X,Y, weight 매트릭스 추출\n",
    "        X_train = XY_train.as_matrix(columns=features)\n",
    "        Y_train = XY_train.as_matrix(columns=[\"y\"])\n",
    "        W_train = XY_train.as_matrix(columns=[\"weight\"])\n",
    "        \n",
    "        # xgboost 데이터로 변환\n",
    "        train = xgb.DMatrix(X_train, label=Y_train, feature_names=features, weight=W_train)\n",
    "        \n",
    "        # 검증 데이터에 대해서 동일한 작업진행\n",
    "        X_validate = XY_validate.as_matrix(columns=features)\n",
    "        Y_validate = XY_validate.as_matrix(columns=[\"y\"])\n",
    "        W_validate = XY_validate.as_matrix(columns=[\"weight\"])\n",
    "        \n",
    "        # xgboost 데이터로 변환\n",
    "        validate = xgb.DMatrix(X_validate, label=Y_validate, feature_names=features, weight=W_validate)\n",
    "        \n",
    "        # XGBoost 모델 학습 - 얼리 스탑 : 20, 트리 갯수 : 1000\n",
    "        evallist = [(train,'train'), (validate,'eval')]\n",
    "        model = xgb.train(param, train, 1000, evals=evallist, early_stopping_rounds=20)\n",
    "        \n",
    "        # 학습 모델 저장\n",
    "        pickle.dump(model, open(\"next_multi.pickle\", \"wb\"))\n",
    "        \n",
    "    else:\n",
    "        # 2016-06-28 테스트 데이터를 사용할 시 사전에 학습된 모델 불러옴\n",
    "        model = pickle.load(open(\"next_multi.pickle\", \"rb\"))\n",
    "    \n",
    "    # 교차 검증으로 최적의 트리 갯수를 정함\n",
    "    best_ntree_limit = model.best_ntree_limit\n",
    "    \n",
    "    if XY_all is not None:\n",
    "        # 전체 훈련 데이터에 대해 X,Y,weight를 추출하고 XGBoost 전용 데이터로 변환\n",
    "        X_all = XY_all.as_matrix(columns = features)\n",
    "        Y_all = XY_all.as_matrix(columns = [\"y\"])\n",
    "        W_all = XY_all.as_matrix(columns = [\"weight\"])\n",
    "        all_data = xgb.DMatrix(X_all, label=Y_all, feature_names=features, weight=W_all)\n",
    "        \n",
    "        evallist = [(all_data, 'all_data')]\n",
    "        \n",
    "        # 학습할 트리 갯수를 전체 훈련 데이터가 늘어난 만큼 조정??????\n",
    "        best_ntree_limit = int(best_ntree_limit * (len(XY_train) + len(XY_validate))/ len(XY_train))\n",
    "        \n",
    "        # 모델 학습!!!\n",
    "        model = xgb.train(param, all_data,best_ntree_limit, evals=evallist)\n",
    "        \n",
    "        \n",
    "    # 변수 중요도 출력 .get_fscore()통해서\n",
    "    print(\"Feature importance : \")\n",
    "    for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key = lambda kv : kv[1], reverse=True) :\n",
    "        print(kv)\n",
    "        \n",
    "    # 예측에 사용할 테스트 데이터를 XGBoost 데이터로 변환, 이때의 웨이트는 전부 1이기에 별도 작업 없음\n",
    "    X_test = test_df.as_matrix(columns = features)\n",
    "    test = xgb.DMatrix(X_test, feature_names=features)\n",
    "    \n",
    "    # 학습된 모델, best_ntree_limit 기반 예측\n",
    "    return model.predict(test, ntree_limit = best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(f, Y_test, C):\n",
    "    Y_ret = []\n",
    "    # 파일 첫 줄에 header\n",
    "    f.write(\"ncodpers, added_products\\n\".encode('utf-8'))\n",
    "    # 고객 식별번호(C), 예측결과물(Y_test) for loop\n",
    "    \n",
    "    for c, y_test in zip(C,Y_test):\n",
    "        # 확률, 금융변수, 금융변수 id 튜플\n",
    "        y_prods = [(y, p , ip) for y, p, ip in zip(y_test, products, range(len(products)))]\n",
    "        \n",
    "        # 확률 기준 상위 7개 결과 추출\n",
    "        y_prods = sortes(y_prods, key = lambda a : a[0], reverse = True)[:7]\n",
    "        \n",
    "        # 금융 id를 Y_ret에 저장\n",
    "        Y_ret.append([ip for y,p,ip in y_prods])\n",
    "        y_prods = [p for y,p, ip in y_prods]\n",
    "        \n",
    "        # 고객식별, 7개 변수 파일에 기재\n",
    "        f.write((\"%s,%s\\n\" % (int(c), \" \".join(y_prods))).encode('utf-8'))\n",
    "        \n",
    "    return Y_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(all_df, features, prod_features, str_date, cv):\n",
    "    \n",
    "    # str_date로 예측 결과물을 산출하는 날짜 지정\n",
    "    test_date = date_to_int(str_date)\n",
    "    \n",
    "    # 훈련 데이터는 test_date 이전의 모든데이터\n",
    "    train_df = all_df[all_df.int_date < test_date]\n",
    "    # 테스트 데이터를 분리\n",
    "    test_df = pd.DataFrame(all_df[all_df.int_date == test_date])\n",
    "    \n",
    "    \n",
    "    # 신규 구매 고객만을 훈련 데이터로 추출\n",
    "    X =[]\n",
    "    Y = []\n",
    "    \n",
    "    for i, prod in enumerate(products):\n",
    "        prev = prod + \"_prev1\"\n",
    "        # 신규 구매 고객을 prX에 저장\n",
    "        prX = train_df[(train_df[prod] == 1) & (train_df[prev] == 0)]\n",
    "        # 신규 구매에 대한 label값을 prY에 저장\n",
    "        prY = np.zeros(prX.shape[0], dtype = np.int8) + i\n",
    "        X.append(prX)\n",
    "        Y.append(prY)\n",
    "        \n",
    "        \n",
    "    XY = pd.concat(X)\n",
    "    Y = np.hstack(Y)\n",
    "    \n",
    "    # XY에는 신규 구매 데이터만 포함함\n",
    "    XY[\"y\"] = Y\n",
    "    \n",
    "    # 메모리에서 변수 삭제\n",
    "    del train_df\n",
    "    del all_df\n",
    "    \n",
    "    # 데이터별 가중치 계산위한 변수 (ncodpers  + fecha_dato) 생성\n",
    "    XY[\"ncodpers_fecha_dato\"] = XY[\"ncodpers\"].astype(str) + XY[\"fecha_dato\"]\n",
    "    uniqs, counts = np.unique(XY[\"ncodpers_fecha_dato\"], return_counts= True)\n",
    "    \n",
    "    # 자연상수를 통해서 count가 높은 데이터에 낮은 가중치\n",
    "    weights = np.exp(1/counts - 1)\n",
    "    \n",
    "    # 가중치를 XY데이터에 추가\n",
    "    wdf = pd.DataFrame()\n",
    "    wdf[\"ncodpers_fecha_dato\"] = uniqs\n",
    "    wdf[\"counts\"] = counts\n",
    "    wdf[\"weight\"] = weights\n",
    "    XY = XY.merge(wdf, on=\"ncodpers_fecha_dato\")\n",
    "    \n",
    "    # 교차검증을 위해 8:2로 분리\n",
    "    mask = np.random.rand(len(XY)) < 0.8\n",
    "    XY_train = XY[mask]\n",
    "    XY_validate = XY[~mask]\n",
    "    \n",
    "    \n",
    "    # 테스트 데이터 가중치 전부 1\n",
    "    test_df[\"weight\"] = np.ones(len(test_df), dtype=np.int8)\n",
    "    \n",
    "    # 테스트 데이터에서 신규 구매 정답값을 추출\n",
    "    test_df[\"y\"] = test_df[\"ncodpers\"]\n",
    "    Y_prev = test_df.as_matrix(columns = prod_features)\n",
    "    for prod in products:\n",
    "        prev = prod + \"_prev1\"\n",
    "        padd = prod + \"_add\"\n",
    "        # 신규구매여부\n",
    "        test_df[padd] = test_df[prod] - test_df[prev]\n",
    "    test_add_mat = test_df.as_matrix(columns = [prod + \"_add\" for prod in products])\n",
    "        \n",
    "    C = test_df.as_matrix(columns=[\"ncodpers\"])\n",
    "    test_add_list = [list() for i in range(len(C))]\n",
    "    # MAP@7 계산을 위해 고객별 신규 구매 정답값을 test_add_list에 기록\n",
    "    count = 0\n",
    "    for c in range(len(C)):\n",
    "        for p in range(len(products)):\n",
    "            if test_add_mat[c, p] >0: # 즉 신규\n",
    "                test_add_list[c].append(p)\n",
    "                count +=1\n",
    "                \n",
    "    # 교차 검증에서, 테스트 데이터로 분리된 데이터가 얻을 수 있는 최대 MAP@7 값을 계산한다. \n",
    "    if cv:\n",
    "        max_map7 = mapk(test_add_list, test_add_list, 7, 0.0)\n",
    "        map7coef = float(len(test_add_list)) / float(sum([int(bool(a)) for a in test_add_list]))\n",
    "        print(\"Max MAP@7\", str_date, max_map7, max_map7*map7coef)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #XGBoost 모델 학습 후 예측 결과물 저장\n",
    "    Y_test_xgb = xgboost(XY_train, XY_validate, test_df, features, XY_all = XY, restore=(str_date == \"2016-06-28\"))\n",
    "    test_add_list_xgboost = make_submission(io.BytesIO() if cv else gzip.open(\"%s.xgboost.csv.gz\" % str_date, \"wb\"). Y_test_xgb - Y_prev, C) #Y_prev를 빼면 신규 구매가 아닌것의 확률을 확 낮춤\n",
    "    \n",
    "    # 교차 검증일 시 XGBoost 모델의 테스트 데이터 MAP@7 척도 출력\n",
    "    \n",
    "    if cv:\n",
    "        map7xgboost = mapk(test_add_list, test_add_list_xgboost, 7, 0.0)\n",
    "        print(\"XGBoost MAP@7\", str_date, map7xgboost, map7xgboost* map7coef)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2963: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n",
      "/home/jeongchanwoo/.local/lib/python3.5/site-packages/numpy/lib/nanfunctions.py:1434: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:27: RuntimeWarning: All-NaN slice encountered\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:30: RuntimeWarning: All-NaN slice encountered\n"
     ]
    }
   ],
   "source": [
    "all_df , features, prod_features = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle(\"/home/jeongchanwoo/workspace/git/study/Kaggle_data/santander-product-recommendation/input/8th.feature_engineer.all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((features, prod_features), open(\"/home/jeongchanwoo/workspace/git/study/Kaggle_data/santander-product-recommendation/input/8th.feature_engineer.cv_meta.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:61: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:67: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:69: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max MAP@7 2016-05-28 0.030081054277552897 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:19: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:27: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:28: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:29: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.55252\teval-mlogloss:2.55707\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:2.28405\teval-mlogloss:2.29052\n",
      "[2]\ttrain-mlogloss:2.09588\teval-mlogloss:2.10389\n",
      "[3]\ttrain-mlogloss:1.95057\teval-mlogloss:1.95981\n",
      "[4]\ttrain-mlogloss:1.83282\teval-mlogloss:1.84325\n",
      "[5]\ttrain-mlogloss:1.7344\teval-mlogloss:1.74583\n",
      "[6]\ttrain-mlogloss:1.65061\teval-mlogloss:1.66306\n",
      "[7]\ttrain-mlogloss:1.57815\teval-mlogloss:1.59137\n",
      "[8]\ttrain-mlogloss:1.51462\teval-mlogloss:1.52862\n",
      "[9]\ttrain-mlogloss:1.45848\teval-mlogloss:1.47325\n",
      "[10]\ttrain-mlogloss:1.40904\teval-mlogloss:1.42453\n",
      "[11]\ttrain-mlogloss:1.36487\teval-mlogloss:1.38108\n",
      "[12]\ttrain-mlogloss:1.32499\teval-mlogloss:1.34187\n",
      "[13]\ttrain-mlogloss:1.28917\teval-mlogloss:1.30674\n",
      "[14]\ttrain-mlogloss:1.25654\teval-mlogloss:1.27469\n",
      "[15]\ttrain-mlogloss:1.22696\teval-mlogloss:1.2457\n",
      "[16]\ttrain-mlogloss:1.20033\teval-mlogloss:1.21965\n",
      "[17]\ttrain-mlogloss:1.17584\teval-mlogloss:1.19568\n",
      "[18]\ttrain-mlogloss:1.15336\teval-mlogloss:1.17377\n",
      "[19]\ttrain-mlogloss:1.13288\teval-mlogloss:1.15381\n",
      "[20]\ttrain-mlogloss:1.11411\teval-mlogloss:1.13559\n",
      "[21]\ttrain-mlogloss:1.09683\teval-mlogloss:1.11888\n",
      "[22]\ttrain-mlogloss:1.08085\teval-mlogloss:1.10345\n",
      "[23]\ttrain-mlogloss:1.066\teval-mlogloss:1.08909\n",
      "[24]\ttrain-mlogloss:1.0523\teval-mlogloss:1.07595\n",
      "[25]\ttrain-mlogloss:1.03958\teval-mlogloss:1.06376\n",
      "[26]\ttrain-mlogloss:1.02794\teval-mlogloss:1.05262\n",
      "[27]\ttrain-mlogloss:1.01707\teval-mlogloss:1.04236\n",
      "[28]\ttrain-mlogloss:1.00706\teval-mlogloss:1.03286\n",
      "[29]\ttrain-mlogloss:0.997725\teval-mlogloss:1.02403\n",
      "[30]\ttrain-mlogloss:0.989053\teval-mlogloss:1.01585\n",
      "[31]\ttrain-mlogloss:0.981079\teval-mlogloss:1.00839\n",
      "[32]\ttrain-mlogloss:0.973466\teval-mlogloss:1.00127\n",
      "[33]\ttrain-mlogloss:0.966455\teval-mlogloss:0.994803\n",
      "[34]\ttrain-mlogloss:0.959733\teval-mlogloss:0.988605\n",
      "[35]\ttrain-mlogloss:0.953471\teval-mlogloss:0.982854\n",
      "[36]\ttrain-mlogloss:0.947584\teval-mlogloss:0.977471\n",
      "[37]\ttrain-mlogloss:0.941992\teval-mlogloss:0.972374\n",
      "[38]\ttrain-mlogloss:0.936742\teval-mlogloss:0.967641\n",
      "[39]\ttrain-mlogloss:0.931837\teval-mlogloss:0.963293\n",
      "[40]\ttrain-mlogloss:0.927247\teval-mlogloss:0.959215\n",
      "[41]\ttrain-mlogloss:0.92287\teval-mlogloss:0.955379\n",
      "[42]\ttrain-mlogloss:0.918748\teval-mlogloss:0.95175\n",
      "[43]\ttrain-mlogloss:0.914903\teval-mlogloss:0.948395\n",
      "[44]\ttrain-mlogloss:0.911101\teval-mlogloss:0.945069\n",
      "[45]\ttrain-mlogloss:0.907565\teval-mlogloss:0.942082\n",
      "[46]\ttrain-mlogloss:0.904293\teval-mlogloss:0.939346\n",
      "[47]\ttrain-mlogloss:0.90107\teval-mlogloss:0.936644\n",
      "[48]\ttrain-mlogloss:0.898083\teval-mlogloss:0.934164\n",
      "[49]\ttrain-mlogloss:0.895208\teval-mlogloss:0.931792\n",
      "[50]\ttrain-mlogloss:0.892431\teval-mlogloss:0.929497\n",
      "[51]\ttrain-mlogloss:0.88985\teval-mlogloss:0.927402\n",
      "[52]\ttrain-mlogloss:0.887372\teval-mlogloss:0.925413\n",
      "[53]\ttrain-mlogloss:0.88506\teval-mlogloss:0.923584\n",
      "[54]\ttrain-mlogloss:0.882689\teval-mlogloss:0.92174\n",
      "[55]\ttrain-mlogloss:0.880592\teval-mlogloss:0.920146\n",
      "[56]\ttrain-mlogloss:0.878468\teval-mlogloss:0.918528\n",
      "[57]\ttrain-mlogloss:0.876454\teval-mlogloss:0.916993\n",
      "[58]\ttrain-mlogloss:0.874499\teval-mlogloss:0.915535\n",
      "[59]\ttrain-mlogloss:0.872703\teval-mlogloss:0.914219\n",
      "[60]\ttrain-mlogloss:0.87094\teval-mlogloss:0.912925\n",
      "[61]\ttrain-mlogloss:0.86925\teval-mlogloss:0.911761\n",
      "[62]\ttrain-mlogloss:0.867654\teval-mlogloss:0.910649\n",
      "[63]\ttrain-mlogloss:0.86611\teval-mlogloss:0.909555\n",
      "[64]\ttrain-mlogloss:0.864634\teval-mlogloss:0.908561\n",
      "[65]\ttrain-mlogloss:0.86313\teval-mlogloss:0.907479\n",
      "[66]\ttrain-mlogloss:0.861775\teval-mlogloss:0.906535\n",
      "[67]\ttrain-mlogloss:0.860445\teval-mlogloss:0.905633\n",
      "[68]\ttrain-mlogloss:0.859003\teval-mlogloss:0.904719\n",
      "[69]\ttrain-mlogloss:0.857701\teval-mlogloss:0.903876\n",
      "[70]\ttrain-mlogloss:0.856403\teval-mlogloss:0.903071\n",
      "[71]\ttrain-mlogloss:0.855191\teval-mlogloss:0.902364\n",
      "[72]\ttrain-mlogloss:0.854051\teval-mlogloss:0.901658\n",
      "[73]\ttrain-mlogloss:0.852855\teval-mlogloss:0.900999\n",
      "[74]\ttrain-mlogloss:0.85183\teval-mlogloss:0.900385\n",
      "[75]\ttrain-mlogloss:0.850806\teval-mlogloss:0.899804\n",
      "[76]\ttrain-mlogloss:0.849821\teval-mlogloss:0.899262\n",
      "[77]\ttrain-mlogloss:0.848848\teval-mlogloss:0.898716\n",
      "[78]\ttrain-mlogloss:0.847918\teval-mlogloss:0.898216\n",
      "[79]\ttrain-mlogloss:0.847005\teval-mlogloss:0.897715\n",
      "[80]\ttrain-mlogloss:0.846083\teval-mlogloss:0.897234\n",
      "[81]\ttrain-mlogloss:0.845243\teval-mlogloss:0.896834\n",
      "[82]\ttrain-mlogloss:0.844318\teval-mlogloss:0.89636\n",
      "[83]\ttrain-mlogloss:0.843459\teval-mlogloss:0.895976\n",
      "[84]\ttrain-mlogloss:0.842634\teval-mlogloss:0.895633\n",
      "[85]\ttrain-mlogloss:0.841845\teval-mlogloss:0.895243\n",
      "[86]\ttrain-mlogloss:0.841052\teval-mlogloss:0.894894\n",
      "[87]\ttrain-mlogloss:0.840301\teval-mlogloss:0.894598\n",
      "[88]\ttrain-mlogloss:0.839557\teval-mlogloss:0.894295\n",
      "[89]\ttrain-mlogloss:0.838775\teval-mlogloss:0.893975\n",
      "[90]\ttrain-mlogloss:0.837978\teval-mlogloss:0.893725\n",
      "[91]\ttrain-mlogloss:0.837236\teval-mlogloss:0.893413\n",
      "[92]\ttrain-mlogloss:0.83648\teval-mlogloss:0.893124\n",
      "[93]\ttrain-mlogloss:0.835781\teval-mlogloss:0.892885\n",
      "[94]\ttrain-mlogloss:0.835101\teval-mlogloss:0.892617\n",
      "[95]\ttrain-mlogloss:0.834497\teval-mlogloss:0.892381\n",
      "[96]\ttrain-mlogloss:0.83379\teval-mlogloss:0.892127\n",
      "[97]\ttrain-mlogloss:0.833163\teval-mlogloss:0.891953\n",
      "[98]\ttrain-mlogloss:0.832547\teval-mlogloss:0.891751\n",
      "[99]\ttrain-mlogloss:0.831889\teval-mlogloss:0.89159\n",
      "[100]\ttrain-mlogloss:0.831252\teval-mlogloss:0.891404\n",
      "[101]\ttrain-mlogloss:0.83058\teval-mlogloss:0.891228\n",
      "[102]\ttrain-mlogloss:0.829943\teval-mlogloss:0.89109\n",
      "[103]\ttrain-mlogloss:0.82925\teval-mlogloss:0.890879\n",
      "[104]\ttrain-mlogloss:0.828562\teval-mlogloss:0.89068\n",
      "[105]\ttrain-mlogloss:0.827975\teval-mlogloss:0.890589\n",
      "[106]\ttrain-mlogloss:0.827335\teval-mlogloss:0.890431\n",
      "[107]\ttrain-mlogloss:0.826706\teval-mlogloss:0.89028\n",
      "[108]\ttrain-mlogloss:0.826126\teval-mlogloss:0.890146\n",
      "[109]\ttrain-mlogloss:0.825434\teval-mlogloss:0.89002\n",
      "[110]\ttrain-mlogloss:0.8248\teval-mlogloss:0.889881\n",
      "[111]\ttrain-mlogloss:0.824257\teval-mlogloss:0.889792\n",
      "[112]\ttrain-mlogloss:0.823657\teval-mlogloss:0.889667\n",
      "[113]\ttrain-mlogloss:0.823045\teval-mlogloss:0.889539\n",
      "[114]\ttrain-mlogloss:0.822398\teval-mlogloss:0.889466\n",
      "[115]\ttrain-mlogloss:0.821783\teval-mlogloss:0.889386\n",
      "[116]\ttrain-mlogloss:0.821231\teval-mlogloss:0.889364\n",
      "[117]\ttrain-mlogloss:0.820572\teval-mlogloss:0.889346\n",
      "[118]\ttrain-mlogloss:0.819986\teval-mlogloss:0.889287\n",
      "[119]\ttrain-mlogloss:0.819282\teval-mlogloss:0.88924\n",
      "[120]\ttrain-mlogloss:0.818617\teval-mlogloss:0.889165\n",
      "[121]\ttrain-mlogloss:0.817922\teval-mlogloss:0.889094\n",
      "[122]\ttrain-mlogloss:0.817328\teval-mlogloss:0.889067\n",
      "[123]\ttrain-mlogloss:0.816621\teval-mlogloss:0.889007\n",
      "[124]\ttrain-mlogloss:0.816015\teval-mlogloss:0.888947\n",
      "[125]\ttrain-mlogloss:0.815319\teval-mlogloss:0.888867\n",
      "[126]\ttrain-mlogloss:0.814675\teval-mlogloss:0.888813\n",
      "[127]\ttrain-mlogloss:0.814052\teval-mlogloss:0.88878\n",
      "[128]\ttrain-mlogloss:0.813487\teval-mlogloss:0.888823\n",
      "[129]\ttrain-mlogloss:0.812846\teval-mlogloss:0.88881\n",
      "[130]\ttrain-mlogloss:0.812327\teval-mlogloss:0.888834\n",
      "[131]\ttrain-mlogloss:0.811814\teval-mlogloss:0.888849\n",
      "[132]\ttrain-mlogloss:0.811297\teval-mlogloss:0.888859\n",
      "[133]\ttrain-mlogloss:0.810664\teval-mlogloss:0.888812\n",
      "[134]\ttrain-mlogloss:0.809982\teval-mlogloss:0.888789\n",
      "[135]\ttrain-mlogloss:0.809354\teval-mlogloss:0.888775\n",
      "[136]\ttrain-mlogloss:0.808801\teval-mlogloss:0.888749\n",
      "[137]\ttrain-mlogloss:0.808108\teval-mlogloss:0.888714\n",
      "[138]\ttrain-mlogloss:0.807415\teval-mlogloss:0.888668\n",
      "[139]\ttrain-mlogloss:0.806908\teval-mlogloss:0.888698\n",
      "[140]\ttrain-mlogloss:0.806299\teval-mlogloss:0.888719\n",
      "[141]\ttrain-mlogloss:0.805655\teval-mlogloss:0.888694\n",
      "[142]\ttrain-mlogloss:0.805027\teval-mlogloss:0.888691\n",
      "[143]\ttrain-mlogloss:0.804501\teval-mlogloss:0.888716\n",
      "[144]\ttrain-mlogloss:0.803896\teval-mlogloss:0.888739\n",
      "[145]\ttrain-mlogloss:0.803358\teval-mlogloss:0.888724\n",
      "[146]\ttrain-mlogloss:0.802787\teval-mlogloss:0.88872\n",
      "[147]\ttrain-mlogloss:0.802147\teval-mlogloss:0.888665\n",
      "[148]\ttrain-mlogloss:0.801571\teval-mlogloss:0.8887\n",
      "[149]\ttrain-mlogloss:0.800929\teval-mlogloss:0.888708\n",
      "[150]\ttrain-mlogloss:0.800381\teval-mlogloss:0.888726\n",
      "[151]\ttrain-mlogloss:0.799748\teval-mlogloss:0.888767\n",
      "[152]\ttrain-mlogloss:0.799189\teval-mlogloss:0.888812\n",
      "[153]\ttrain-mlogloss:0.798594\teval-mlogloss:0.888861\n",
      "[154]\ttrain-mlogloss:0.797908\teval-mlogloss:0.888852\n",
      "[155]\ttrain-mlogloss:0.797353\teval-mlogloss:0.888886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156]\ttrain-mlogloss:0.796683\teval-mlogloss:0.888932\n",
      "[157]\ttrain-mlogloss:0.796113\teval-mlogloss:0.888958\n",
      "[158]\ttrain-mlogloss:0.79571\teval-mlogloss:0.888997\n",
      "[159]\ttrain-mlogloss:0.795096\teval-mlogloss:0.888983\n",
      "[160]\ttrain-mlogloss:0.794515\teval-mlogloss:0.889001\n",
      "[161]\ttrain-mlogloss:0.793948\teval-mlogloss:0.88901\n",
      "[162]\ttrain-mlogloss:0.79342\teval-mlogloss:0.889028\n",
      "[163]\ttrain-mlogloss:0.792933\teval-mlogloss:0.889048\n",
      "[164]\ttrain-mlogloss:0.792371\teval-mlogloss:0.889112\n",
      "[165]\ttrain-mlogloss:0.791819\teval-mlogloss:0.889155\n",
      "[166]\ttrain-mlogloss:0.791257\teval-mlogloss:0.889153\n",
      "[167]\ttrain-mlogloss:0.790765\teval-mlogloss:0.889195\n",
      "Stopping. Best iteration:\n",
      "[147]\ttrain-mlogloss:0.802147\teval-mlogloss:0.888665\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:50: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:51: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:52: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "train_predict(all_df, features, prod_features, \"2016-05-28\", cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict(all_df, features, prod_features, \"2016-06-28\", cv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_python_linux)",
   "language": "python",
   "name": "ml_python_linux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
