{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import io\n",
    "\n",
    "#파일 압축 용도\n",
    "import gzip\n",
    "import pickle\n",
    "import zlib\n",
    "\n",
    "# 데이터, 배열\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 범주형 수치형 변환\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "np.random.seed(2016)\n",
    "transformers={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = (\n",
    "    \"ind_ahor_fin_ult1\",\n",
    "    \"ind_aval_fin_ult1\",\n",
    "    \"ind_cco_fin_ult1\" ,\n",
    "    \"ind_cder_fin_ult1\",\n",
    "    \"ind_cno_fin_ult1\" ,\n",
    "    \"ind_ctju_fin_ult1\",\n",
    "    \"ind_ctma_fin_ult1\",\n",
    "    \"ind_ctop_fin_ult1\",\n",
    "    \"ind_ctpp_fin_ult1\",\n",
    "    \"ind_deco_fin_ult1\",\n",
    "    \"ind_deme_fin_ult1\",\n",
    "    \"ind_dela_fin_ult1\",\n",
    "    \"ind_ecue_fin_ult1\",\n",
    "    \"ind_fond_fin_ult1\",\n",
    "    \"ind_hip_fin_ult1\" ,\n",
    "    \"ind_plan_fin_ult1\",\n",
    "    \"ind_pres_fin_ult1\",\n",
    "    \"ind_reca_fin_ult1\",\n",
    "    \"ind_tjcr_fin_ult1\",\n",
    "    \"ind_valo_fin_ult1\",\n",
    "    \"ind_viv_fin_ult1\" ,\n",
    "    \"ind_nomina_ult1\"  ,\n",
    "    \"ind_nom_pens_ult1\",\n",
    "    \"ind_recibo_ult1\"  ,\n",
    ")\n",
    "\n",
    "dtypes = {\n",
    "    \"fecha_dato\": str,\n",
    "    \"ncodpers\": int,\n",
    "    \"conyuemp\": str, # Spouse index. 1 if the customer is spouse of an employee\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 인코더 함수\n",
    "def label_encode(df, features, name):\n",
    "    # 데이터 프레임 df의 변수 name값을 모두 string으로 변환\n",
    "    df[name] = df[name].astype('str')\n",
    "    \n",
    "    #이미 라벨 인코더 했던 변수는 trasformer[name]에 있는 라벨 인코더를 재활용\n",
    "    if name in transformers:\n",
    "        df[name] = transformers[name].transform(df[name])\n",
    "        \n",
    "    #처음 나오는 변수는 transformer에 라벨인코더를 저장하고 fit_transfrom으로 라벨인코딩\n",
    "    else:\n",
    "        transformers[name] = LabelEncoder()\n",
    "        df[name] = transformers[name].fit_transform(df[name])\n",
    "        # 라벨인코딩한 변수는 features 리스트에 추가\n",
    "        features.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자체 구현 one hot encoder\n",
    "def custom_one_hot(df, features, name, names, dtype = np.int8, check = False):\n",
    "    for n, val in names.items():\n",
    "        # 신규 변수명을 \"변수명_숫자\" 지정\n",
    "        new_name = \"%s_%s\" % (name, n)\n",
    "        #기존 변수에서 해당고유값 있으면 1 그외는 0 이진변수 생성\n",
    "        df[new_name] = df[name].map(lambda x : 1 if x == val else 0).astype(dtype)\n",
    "        features.append(new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply_transform\n",
    "* 결측값 대체 : 결측 값을 0.0 or 1.0으로 대체\n",
    "* 범주형 데이터 라벨 인코딩\n",
    "* 고빈도 top 100 개를 빈도 순위로 변환 : 고빈도 데이터에 대한 선형관계 추출\n",
    "* 수치형 변수 log 변환\n",
    "* 날짜 데이터 년/월 추출\n",
    "* 날짜간 차이값을 통한 파생변수 : 두 데이터간 날짜값의 차이를 통한 상대적인 거리변수를 생성\n",
    "* 원핫 인코딩 : 범주형 데이터의 표현력을 높이기 위해 오든 고유값을 새로운 이진 변수로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨인코더, 원핫인코더, 유틸.py의 빈도 추출, 날짜 변환을 이용해 \n",
    "# 변수에 대한 전처리와 피쳐 엔지니어링 수행\n",
    "def apply_transforms(train_df):\n",
    "    \n",
    "    # 학습에 사용할 변수를 저장할 features 리스트 생성\n",
    "    features = []\n",
    "    \n",
    "    # 두 변수를 label_encode()\n",
    "    label_encode(train_df, features, \"canal_entrada\")\n",
    "    label_encode(train_df, features, \"pais_residencia\")\n",
    "    \n",
    "    # age의 결측값을 0.0으로 대체하고, 모든 값을 정수로 변환.(내 생각 - renta는 값의 범위가 크기에 log를 위해 1로 채워넣음)\n",
    "    train_df[\"age\"] = train_df[\"age\"].fillna(0.0).astype(np.int16)\n",
    "    features.append(\"age\")\n",
    "    \n",
    "    # renta 결측값을 1.0으로 대체 하고 log를 씌워 정수 변환\n",
    "    train_df[\"renta\"].fillna(1.0, inplace=True)\n",
    "    train_df[\"renta\"] = train_df[\"renta\"].map(math.log)\n",
    "    features.append(\"renta\")\n",
    "    \n",
    "    # 고빈도 100개의 순위 추출\n",
    "    train_df[\"renta_top\"] = encode_top(train_df[\"renta\"])\n",
    "    features.append(\"renta_top\")\n",
    "    \n",
    "    #결측값, 음수를 0으로 대체, 나머지는 +1.0후 정수 변환\n",
    "    train_df[\"antiguedad\"] = train_df[\"antiguedad\"].map(lambda x : 0.0 if x < 0 or math.isnan(x) else  x + 1.0).astype(np.int16)\n",
    "    features.append(\"antiguedad\")\n",
    "    \n",
    "    # 결측값을 0.0으로 대체하고, 정수로 변환\n",
    "    train_df[\"tipodom\"] = train_df[\"tipodom\"].fillna(0.0).astype(np.int8)\n",
    "    features.append(\"tipodom\")\n",
    "    \n",
    "    train_df[\"cod_prov\"] = train_df[\"cod_prov\"].fillna(0.0).astype(np.int8)\n",
    "    features.append(\"cod_prov\")\n",
    "    \n",
    "    # fecha_dato에서 월/년도 추출하여 정수값으로 변환\n",
    "    train_df[\"fecha_dato_month\"] = train_df[\"fecha_dato\"].map(lambda x : int(x.split(\"-\")[1])).astype(np.int8)\n",
    "    features.append(\"fecha_dato_month\")\n",
    "    train_df[\"fecha_dato_year\"] = train_df[\"fecha_dato\"].map(lambda x : int(x.split(\"-\")[0])).astype(np.int16)\n",
    "    features.append(\"fecha_dato_year\")\n",
    "    \n",
    "    \n",
    "    # 결측값을 0.0으로 대체하고, fecha_alta에서 월/년도를 추출하여 정수값으로 변환\n",
    "    # x.__class__를 통해 결측값 탐지.  x.__class__는 결측값일 경우 float을 반환\n",
    "    train_df[\"fecha_alta_month\"] = train_df[\"fecha_alta\"].map(lambda x : 0.0 if x.__class__ is float else float(x.split(\"-\")[1])).astype(np.int8)\n",
    "    features.append(\"fecha_alta_month\")\n",
    "    train_df[\"fecha_alta_year\"] = train_df[\"fecha_alta\"].map(lambda x : 0.0 if x.__class__ is float else float(x.split(\"-\")[0])).astype(np.int8)\n",
    "    features.append(\"fecha_alta_year\")\n",
    "    \n",
    "    #날짜 데이터를 월 기준 수치형 변수로 변환\n",
    "    train_df[\"fecha_dato_float\"] = train_df[\"fecha_dato\"].map(date_to_float)\n",
    "    train_df[\"fecha_alta_float\"] = train_df[\"fecha_dato\"].map(date_to_float)\n",
    "    \n",
    "    # fecha_dato와 fecha_alto의 월 기준 수치형 변수의 차이값을 파생 변수로 생성\n",
    "    train_df[\"dato_minus_alta\"] = train_df[\"fecha_dato_float\"] - train_df[\"fecha_alta_float\"]\n",
    "    features.append(\"dato_minus_alta\")\n",
    "    \n",
    "    # 날짜 데이터를 월 기준 수치형 변수로 변환 (1 ~ 18 사이 값으로 제한)\n",
    "    train_df[\"int_date\"] = train_df[\"fecha_dato\"].map(date_to_int).astype(np.int8)\n",
    "    \n",
    "    # 원핫 인코딩 수행\n",
    "    custom_one_hot(train_df, features, \"indresi\", {\"n\" : \"N\"})\n",
    "    custom_one_hot(train_df, features, \"indext\", {\"s\" : \"S\"})\n",
    "    custom_one_hot(train_df, features, \"conyuemp\", {\"n\" : \"N\"})\n",
    "    custom_one_hot(train_df, features, \"sexo\", {\"h\" : \"H\", \"v\":\"V\"})\n",
    "    custom_one_hot(train_df, features, \"ind_empleado\", {\"a\" : \"A\", \"b\":\"B\", \"f\": \"F\", \"n\":\"N\"})\n",
    "    custom_one_hot(train_df, features, \"ind_nuevo\", {\"new\" : \"1\"})\n",
    "    custom_one_hot(train_df, features, \"segmento\", {\"top\" : \"01 - TOP\", \"particulares\" : \"02 - PARTICULARES\", \"universitario\" : \"03 - UNIVERSITARIO\"})\n",
    "    custom_one_hot(train_df, features, \"indfall\", {\"s\" : \"S\"})\n",
    "    custom_one_hot(train_df, features, \"indrel\", {\"1\" : 1, \"99\" : 99})\n",
    "    custom_one_hot(train_df, features, \"tiprel_1mes\", {\"a\" : \"A\", \"i\":\"I\", \"p\":\"P\", \"r\":\"R\"})\n",
    "    \n",
    "    # 결측값을 0.0으로 대체 하고 그외는 +1.0 더하고 정수 변환\n",
    "    train_df[\"ind_actividad_cliente\"] = train_df[\"ind_actividad_cliente\"].map(lambda x : 0.0 if math.isnan(x) else x + 1.0).astype(np.int8)\n",
    "    features.append(\"ind_actividad_cliente\")\n",
    "    \n",
    "    \n",
    "    # 결측값을 0.0으로 대체하고, \"P\"를 5로 대체하고, 정수 변환\n",
    "    train_df[\"indrel_1mes\"] = train_df[\"indrel_1mes\"].map(lambda x : 5.0 if x ==\"P\" else x).astype(float).fillna(0.0).astype(np.int8)\n",
    "    features.append(\"indrel_1mes\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 전처리, 피처 엔지니어링이 1차적으로 완료된 데이터 프레임 train_df와 학습에 필요한 변수리스트 features를 튜플 형태로 반환\n",
    "    return train_df, tuple(features)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag 파생변수 생성\n",
    "* 시계열 문제의 고오오급 파생변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prev_df(train_df, step):\n",
    "    # 새로운 데이터 프레임에 ncodpers를 추가, int_date를 step만큼 이동시킨 값\n",
    "    prev_df = pd.DataFrame()\n",
    "    prev_df[\"ncodpers\"] = train_df[\"ncodpers\"]\n",
    "    \n",
    "    prev_df[\"int_date\"] = train_df[\"int_date\"].map(lambda x : x + step).astype(np.int8)\n",
    "    \n",
    "    # \"변수명_prev1\" 형태의 lag 변수를 생성\n",
    "    prod_features = [\"%s_prev%s\" % (prod, step) for prod in products]\n",
    "    for prod, prev in zip(products, prod_features):\n",
    "        prev_df[prev] = train_df[prod]\n",
    "        \n",
    "    return prev_df, tuple(prod_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag 변수를 훈련 데이터에 통합\n",
    "def join_with_prev(df, prev_df, how):\n",
    "    # merge를 통해 join\n",
    "    df = df.merge(prev_df, on=[\"ncodpers\", \"int_date\"], how = how)\n",
    "    # 24개 금융변수를 소수형으로 변환\n",
    "    for f in set(prev_df.columns.values.tolist()) - set([\"ncodpers\", \"int_date\"]):\n",
    "        df[f] = df[f].astype(np.float16)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    fname = \"/home/jeongchanwoo/workspace/git/study/Kaggle_data/santander-product-recommendation/input/8th.clean.all.csv\"\n",
    "    train_df = pd.read_csv(fname, dtype=dtypes)\n",
    "    \n",
    "    for prod in products:\n",
    "        train_df[prod] = train_df[prod].fillna(0.0).astype(np.int8)\n",
    "        \n",
    "    # 48개 변수 피쳐 엔지니어링\n",
    "    train_df, features = apply_transforms(train_df)\n",
    "    \n",
    "    \n",
    "    # lag_5 변수 생성\n",
    "    prev_dfs = []\n",
    "    prod_features = None\n",
    "    \n",
    "    user_features = frozenset([1,2])\n",
    "    \n",
    "    # 1~5까지의 step에 대해 lag-n 데이터 생성\n",
    "    for step in range(1,6):\n",
    "        prev1_train_df, prod1_features = make_prev_df(train_df, step)\n",
    "        \n",
    "        # 생성한 lag는 prev_dfs에 저장\n",
    "        prev_dfs.append(prev1_train_df)\n",
    "        # features에 lag-1,2만 추가\n",
    "        if step in user_features:\n",
    "            features += prod1_features\n",
    "            \n",
    "        # prod_features에 lag-1 변수명만 저장\n",
    "        if step == 1:\n",
    "            prod_features = prod1_features\n",
    "            \n",
    "    return train_df, prev_dfs, features, prod_features\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    train_df, prev_dfs, features, prod_features = load_data()\n",
    "    # lag-5 변수 통합\n",
    "    for i, prev_df in enumerate(prev_dfs):\n",
    "        how = \"inner\" if i==0 else \"left\"\n",
    "        train_df = join_with_prev(train_df, prev_df, how=how)\n",
    "        \n",
    "    # 24개 금융변수에 대해 lag 별로 기초통계량을 변수화\n",
    "    for prod in products:\n",
    "        # [1~3], [1~5],[2~5] 3개 구간에 대해 표준편차\n",
    "        for begin, end in [(1,3), (1,5), (2,5)]:\n",
    "            prods = [\"%s_prev%s\" % (prod, i ) for i in range(begin - end + 1)]\n",
    "            mp_df = train_df.as_matrix(columns = prods)\n",
    "            stdf = \"%s_std_%s_%s\" % (prod, begin, end)\n",
    "            \n",
    "            # np.nanstd로 표준편차 구하고, features에 신규 파생변수 이름 추가\n",
    "            train_df[stdf] = np.nanstd(mp_df, axis=1)\n",
    "            features += (stdf,)\n",
    "            \n",
    "            \n",
    "        # [2~3], [2~5]에 대해 최솟값 최댓값 구함\n",
    "        for begin, end in [(2,3), (2,5)]:\n",
    "            prods = [\"%s_prev%s\" % (prod, i) for i in range(begin, end+1)]\n",
    "            mp_df = train_df.as_matrix(columns = prods)\n",
    "            \n",
    "            minf = \"%s_min_%s_%s\" % (prod, begin,end)\n",
    "            train_df[minf] = np.nanmin(mp_df, axis=1).astype(np.int8)\n",
    "            \n",
    "            maxf = \"%s_max_%s_%s\" % (prod, begin,end)\n",
    "            train_df[maxf] = np.nanmax(mp_df, axis=1).astype(np.int8)\n",
    "            \n",
    "            features += (minf, maxf,)\n",
    "            \n",
    "            \n",
    "    # 고객 식별 번호(ncodpers), 정수 표현 날짜(inf_date), 실제 날짜(fecha_dato), 24개 금융변수(prodcuts), 전처리/피처 엔지니어링 변수(features)가 주요변수\n",
    "    \n",
    "    leave_columns = [\"ncodpers\", \"int_date\", \"fecha_dato\"] + list(products) + list(features)\n",
    "    \n",
    "    # 중복값 확인\n",
    "    assert len(leave_columns) == len(set(leave_columns))\n",
    "    \n",
    "    # train_df에서 주요변수 추출\n",
    "    train_df = train_df[leave_columns]\n",
    "    \n",
    "    return train_df, features, prod_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도 상위 100개 데이터 순위 변수 추출\n",
    "def encode_top(s, count=100, dtype = np.int8):\n",
    "    # 고유값 빈도 계산\n",
    "    uniqs, freqs = np.unique(s, return_counts=True)\n",
    "    # 빈도 top 100 추출\n",
    "    top = sorted(zip(uniqs, freqs), key = lambda vk : vk[1], reverse= True)[:count]\n",
    "    \n",
    "    # 기존데이터 : 순위 dict 생성\n",
    "    top_map = {uf[0] : l + 1 for uf, l in zip(top, range(len(top)))}\n",
    "    \n",
    "    # 고빈도 100개의 데이터는 순위로 대체, 그외는 0으로 대체\n",
    "    return s.map(lambda x: top_map.get(x,0)).astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#날짜데이터를 월 단위 숫자로 변환 utils.py\n",
    "\n",
    "def date_to_float(str_date):\n",
    "    if str_date.__class__ is float and math.isnan(str_date) or str_date ==\"\":\n",
    "        return np.nan\n",
    "\n",
    "    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n",
    "    float_date = float(Y) * 12 + float(M)\n",
    "    return float_date\n",
    "\n",
    "\n",
    "#날짜데이터를 월단위로 변환하여 1~18사이로 제한\n",
    "def date_to_int(str_date):\n",
    "    Y,M,D = [int(a) for a in str_date.strip().split(\"-\")]\n",
    "    int_date = (int(Y) - 2015) * 12 + int(M)\n",
    "    assert 1 <= int_date <= 12+6\n",
    "    return int_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=10, default=1.0):\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return default\n",
    "\n",
    "    return score / min(len(actual), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk(actual, predicted, k=10, default=1.0):\n",
    "    return np.mean([apk(a,p,k,default) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', \n",
    "                  booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, \n",
    "                  subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, \n",
    "                  base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost 모델 학습\n",
    "def xgboost(XY_train, XY_validate, test_df, features, XY_all = None, restore = False):\n",
    "    # 최적 파라메터\n",
    "    param = {\n",
    "        'objective' : 'multi:softprob',\n",
    "        'eta' : 0.1,\n",
    "        'min_child_weight' : 10,\n",
    "        'max_depth' : 8,\n",
    "        'silent' : 1,\n",
    "        'eval_metric' : 'mlogloss',\n",
    "        'colsample_bytree' : 0.8,\n",
    "        'colsample_bylevel' : 0.9,\n",
    "        'num_class' : len(products),\n",
    "        'tree_method' : 'gpu_exact',\n",
    "    }\n",
    "    \n",
    "    if not restore:\n",
    "        # 훈련 데이터에서 X,Y, weight 매트릭스 추출\n",
    "        X_train = XY_train.as_matrix(columns=features)\n",
    "        Y_train = XY_train.as_matrix(columns=[\"y\"])\n",
    "        W_train = XY_train.as_matrix(columns=[\"weight\"])\n",
    "        \n",
    "        # xgboost 데이터로 변환\n",
    "        train = xgb.DMatrix(X_train, label=Y_train, feature_names=features, weight=W_train)\n",
    "        \n",
    "        # 검증 데이터에 대해서 동일한 작업진행\n",
    "        X_validate = XY_validate.as_matrix(columns=features)\n",
    "        Y_validate = XY_validate.as_matrix(columns=[\"y\"])\n",
    "        W_validate = XY_validate.as_matrix(columns=[\"weight\"])\n",
    "        \n",
    "        # xgboost 데이터로 변환\n",
    "        validate = xgb.DMatrix(X_validate, label=Y_validate, feature_names=features, weight=W_validate)\n",
    "        \n",
    "        # XGBoost 모델 학습 - 얼리 스탑 : 20, 트리 갯수 : 1000\n",
    "        evallist = [(train,'train'), (validate,'eval')]\n",
    "        model = xgb.train(param, train, 1000, evals=evallist, early_stopping_rounds=20)\n",
    "        \n",
    "        # 학습 모델 저장\n",
    "        pickle.dump(model, open(\"next_multi.pickle\", \"wb\"))\n",
    "        \n",
    "    else:\n",
    "        # 2016-06-28 테스트 데이터를 사용할 시 사전에 학습된 모델 불러옴\n",
    "        model = pickle.load(open(\"next_multi.pickle\", \"rb\"))\n",
    "    \n",
    "    # 교차 검증으로 최적의 트리 갯수를 정함\n",
    "    best_ntree_limit = model.best_ntree_limit\n",
    "    \n",
    "    if XY_all is not None:\n",
    "        # 전체 훈련 데이터에 대해 X,Y,weight를 추출하고 XGBoost 전용 데이터로 변환\n",
    "        X_all = XY_all.as_matrix(columns = features)\n",
    "        Y_all = XY_all.as_matrix(columns = [\"y\"])\n",
    "        W_all = XY_all.as_matrix(columns = [\"weight\"])\n",
    "        all_data = xgb.DMatrix(X_all, label=Y_all, feature_names=features, weight=W_all)\n",
    "        \n",
    "        evallist = [(all_data, 'all_data')]\n",
    "        \n",
    "        # 학습할 트리 갯수를 전체 훈련 데이터가 늘어난 만큼 조정??????\n",
    "        best_ntree_limit = int(best_ntree_limit * (len(XY_train) + len(XY_validate))/ len(XY_train))\n",
    "        \n",
    "        # 모델 학습!!!\n",
    "        model = xgb.train(param, all_data,best_ntree_limit, evals=evallist)\n",
    "        \n",
    "        \n",
    "    # 변수 중요도 출력 .get_fscore()통해서\n",
    "    print(\"Feature importance : \")\n",
    "    for kv in sorted([(k,v) for k,v in model.get_fscore().items()], key = lambda kv : kv[1], reverse=True) :\n",
    "        print(kv)\n",
    "        \n",
    "    # 예측에 사용할 테스트 데이터를 XGBoost 데이터로 변환, 이때의 웨이트는 전부 1이기에 별도 작업 없음\n",
    "    X_test = test_df.as_matrix(columns = features)\n",
    "    test = xgb.DMatrix(X_test, feature_names=features)\n",
    "    \n",
    "    # 학습된 모델, best_ntree_limit 기반 예측\n",
    "    return model.predict(test, ntree_limit = best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(f, Y_test, C):\n",
    "    Y_ret = []\n",
    "    # 파일 첫 줄에 header\n",
    "    f.write(\"ncodpers, added_products\\n\".encode('utf-8'))\n",
    "    # 고객 식별번호(C), 예측결과물(Y_test) for loop\n",
    "    \n",
    "    for c, y_test in zip(C,Y_test):\n",
    "        # 확률, 금융변수, 금융변수 id 튜플\n",
    "        y_prods = [(y, p , ip) for y, p, ip in zip(y_test, products, range(len(products)))]\n",
    "        \n",
    "        # 확률 기준 상위 7개 결과 추출\n",
    "        y_prods = sortes(y_prods, key = lambda a : a[0], reverse = True)[:7]\n",
    "        \n",
    "        # 금융 id를 Y_ret에 저장\n",
    "        Y_ret.append([ip for y,p,ip in y_prods])\n",
    "        y_prods = [p for y,p, ip in y_prods]\n",
    "        \n",
    "        # 고객식별, 7개 변수 파일에 기재\n",
    "        f.write((\"%s,%s\\n\" % (int(c), \" \".join(y_prods))).encode('utf-8'))\n",
    "        \n",
    "    return Y_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(all_df, features, prod_features, str_date, cv):\n",
    "    \n",
    "    # str_date로 예측 결과물을 산출하는 날짜 지정\n",
    "    test_date = date_to_int(str_date)\n",
    "    \n",
    "    # 훈련 데이터는 test_date 이전의 모든데이터\n",
    "    train_df = all_df[all_df.int_date < test_date]\n",
    "    # 테스트 데이터를 분리\n",
    "    test_df = pd.DataFrame(all_df[all_df.int_date == test_date])\n",
    "    \n",
    "    \n",
    "    # 신규 구매 고객만을 훈련 데이터로 추출\n",
    "    X =[]\n",
    "    Y = []\n",
    "    \n",
    "    for i, prod in enumerate(products):\n",
    "        prev = prod + \"_prev1\"\n",
    "        # 신규 구매 고객을 prX에 저장\n",
    "        prX = train_df[(train_df[prod] == 1) & (train_df[prev] == 0)]\n",
    "        # 신규 구매에 대한 label값을 prY에 저장\n",
    "        prY = np.zeros(prX.shape[0], dtype = np.int8) + i\n",
    "        X.append(prX)\n",
    "        Y.append(prY)\n",
    "        \n",
    "        \n",
    "    XY = pd.concat(X)\n",
    "    Y = np.hstack(Y)\n",
    "    \n",
    "    # XY에는 신규 구매 데이터만 포함함\n",
    "    XY[\"y\"] = Y\n",
    "    \n",
    "    # 메모리에서 변수 삭제\n",
    "    del train_df\n",
    "    del all_df\n",
    "    \n",
    "    # 데이터별 가중치 계산위한 변수 (ncodpers  + fecha_dato) 생성\n",
    "    XY[\"ncodpers_fecha_dato\"] = XY[\"ncodpers\"].astype(str) + XY[\"fecha_dato\"]\n",
    "    uniqs, counts = np.unique(XY[\"ncodpers_fecha_dato\"], return_counts= True)\n",
    "    \n",
    "    # 자연상수를 통해서 count가 높은 데이터에 낮은 가중치\n",
    "    weights = np.exp(1/counts - 1)\n",
    "    \n",
    "    # 가중치를 XY데이터에 추가\n",
    "    wdf = pd.DataFrame()\n",
    "    wdf[\"ncodpers_fecha_dato\"] = uniqs\n",
    "    wdf[\"counts\"] = counts\n",
    "    wdf[\"weight\"] = weights\n",
    "    XY = XY.merge(wdf, on=\"ncodpers_fecha_dato\")\n",
    "    \n",
    "    # 교차검증을 위해 8:2로 분리\n",
    "    mask = np.random.rand(len(XY)) < 0.8\n",
    "    XY_train = XY[mask]\n",
    "    XY_validate = XY[~mask]\n",
    "    \n",
    "    \n",
    "    # 테스트 데이터 가중치 전부 1\n",
    "    test_df[\"weight\"] = np.ones(len(test_df), dtype=np.int8)\n",
    "    \n",
    "    # 테스트 데이터에서 신규 구매 정답값을 추출\n",
    "    test_df[\"y\"] = test_df[\"ncodpers\"]\n",
    "    Y_prev = test_df.as_matrix(columns = prod_features)\n",
    "    for prod in products:\n",
    "        prev = prod + \"_prev1\"\n",
    "        padd = prod + \"_add\"\n",
    "        # 신규구매여부\n",
    "        test_df[padd] = test_df[prod] - test_df[prev]\n",
    "    test_add_mat = test_df.as_matrix(columns = [prod + \"_add\" for prod in products])\n",
    "        \n",
    "    C = test_df.as_matrix(columns=[\"ncodpers\"])\n",
    "    test_add_list = [list() for i in range(len(C))]\n",
    "    # MAP@7 계산을 위해 고객별 신규 구매 정답값을 test_add_list에 기록\n",
    "    count = 0\n",
    "    for c in range(len(C)):\n",
    "        for p in range(len(products)):\n",
    "            if test_add_mat[c, p] >0: # 즉 신규\n",
    "                test_add_list[c].append(p)\n",
    "                count +=1\n",
    "                \n",
    "    # 교차 검증에서, 테스트 데이터로 분리된 데이터가 얻을 수 있는 최대 MAP@7 값을 계산한다. \n",
    "    if cv:\n",
    "        max_map7 = mapk(test_add_list, test_add_list, 7, 0.0)\n",
    "        map7coef = float(len(test_add_list)) / float(sum([int(bool(a)) for a in test_add_list]))\n",
    "        print(\"Max MAP@7\", str_date, max_map7, max_map7*map7coef)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #XGBoost 모델 학습 후 예측 결과물 저장\n",
    "    Y_test_xgb = xgboost(XY_train, XY_validate, test_df, features, XY_all = XY, restore=(str_date == \"2016-06-28\"))\n",
    "    test_add_list_xgboost = make_submission(io.BytesIO() if cv else gzip.open(\"%s.xgboost.csv.gz\" % str_date, \"wb\"). Y_test_xgb - Y_prev, C) #Y_prev를 빼면 신규 구매가 아닌것의 확률을 확 낮춤\n",
    "    \n",
    "    # 교차 검증일 시 XGBoost 모델의 테스트 데이터 MAP@7 척도 출력\n",
    "    \n",
    "    if cv:\n",
    "        map7xgboost = mapk(test_add_list, test_add_list_xgboost, 7, 0.0)\n",
    "        print(\"XGBoost MAP@7\", str_date, map7xgboost, map7xgboost* map7coef)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2963: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n",
      "/home/jeongchanwoo/.local/lib/python3.5/site-packages/numpy/lib/nanfunctions.py:1434: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:27: RuntimeWarning: All-NaN slice encountered\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:30: RuntimeWarning: All-NaN slice encountered\n"
     ]
    }
   ],
   "source": [
    "all_df , features, prod_features = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_pickle(\"/home/jeongchanwoo/workspace/git/study/Kaggle_data/santander-product-recommendation/input/8th.feature_engineer.all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((features, prod_features), open(\"/home/jeongchanwoo/workspace/git/study/Kaggle_data/santander-product-recommendation/input/8th.feature_engineer.cv_meta.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:61: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:67: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:69: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max MAP@7 2016-05-28 0.030081054277552897 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:19: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:27: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:28: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/jeongchanwoo/miniconda3/envs/ml_python_linux/lib/python3.5/site-packages/ipykernel_launcher.py:29: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.55359\teval-mlogloss:2.55433\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-mlogloss:2.28507\teval-mlogloss:2.28668\n",
      "[2]\ttrain-mlogloss:2.09763\teval-mlogloss:2.09994\n",
      "[3]\ttrain-mlogloss:1.95214\teval-mlogloss:1.95522\n",
      "[4]\ttrain-mlogloss:1.83454\teval-mlogloss:1.83834\n",
      "[5]\ttrain-mlogloss:1.73627\teval-mlogloss:1.7407\n",
      "[6]\ttrain-mlogloss:1.65245\teval-mlogloss:1.65763\n",
      "[7]\ttrain-mlogloss:1.58\teval-mlogloss:1.58577\n",
      "[8]\ttrain-mlogloss:1.51668\teval-mlogloss:1.52304\n",
      "[9]\ttrain-mlogloss:1.46083\teval-mlogloss:1.46775\n",
      "[10]\ttrain-mlogloss:1.41136\teval-mlogloss:1.41894\n",
      "[11]\ttrain-mlogloss:1.36723\teval-mlogloss:1.37538\n",
      "[12]\ttrain-mlogloss:1.32716\teval-mlogloss:1.33595\n",
      "[13]\ttrain-mlogloss:1.29099\teval-mlogloss:1.3004\n",
      "[14]\ttrain-mlogloss:1.25859\teval-mlogloss:1.26858\n",
      "[15]\ttrain-mlogloss:1.22897\teval-mlogloss:1.2396\n",
      "[16]\ttrain-mlogloss:1.20223\teval-mlogloss:1.21351\n",
      "[17]\ttrain-mlogloss:1.1778\teval-mlogloss:1.18969\n",
      "[18]\ttrain-mlogloss:1.15564\teval-mlogloss:1.16811\n",
      "[19]\ttrain-mlogloss:1.13508\teval-mlogloss:1.14821\n",
      "[20]\ttrain-mlogloss:1.11625\teval-mlogloss:1.12998\n",
      "[21]\ttrain-mlogloss:1.09909\teval-mlogloss:1.11344\n",
      "[22]\ttrain-mlogloss:1.08305\teval-mlogloss:1.09795\n",
      "[23]\ttrain-mlogloss:1.06823\teval-mlogloss:1.08371\n",
      "[24]\ttrain-mlogloss:1.05448\teval-mlogloss:1.07051\n",
      "[25]\ttrain-mlogloss:1.04197\teval-mlogloss:1.05852\n",
      "[26]\ttrain-mlogloss:1.03029\teval-mlogloss:1.04743\n",
      "[27]\ttrain-mlogloss:1.01968\teval-mlogloss:1.03733\n",
      "[28]\ttrain-mlogloss:1.00984\teval-mlogloss:1.02801\n",
      "[29]\ttrain-mlogloss:1.00057\teval-mlogloss:1.01932\n",
      "[30]\ttrain-mlogloss:0.992027\teval-mlogloss:1.01131\n",
      "[31]\ttrain-mlogloss:0.983965\teval-mlogloss:1.00383\n",
      "[32]\ttrain-mlogloss:0.976351\teval-mlogloss:0.996839\n",
      "[33]\ttrain-mlogloss:0.969246\teval-mlogloss:0.990299\n",
      "[34]\ttrain-mlogloss:0.962685\teval-mlogloss:0.984274\n",
      "[35]\ttrain-mlogloss:0.956378\teval-mlogloss:0.978532\n",
      "[36]\ttrain-mlogloss:0.950483\teval-mlogloss:0.973239\n",
      "[37]\ttrain-mlogloss:0.945011\teval-mlogloss:0.968323\n",
      "[38]\ttrain-mlogloss:0.93981\teval-mlogloss:0.963656\n",
      "[39]\ttrain-mlogloss:0.934834\teval-mlogloss:0.959237\n",
      "[40]\ttrain-mlogloss:0.930124\teval-mlogloss:0.955043\n",
      "[41]\ttrain-mlogloss:0.92579\teval-mlogloss:0.951225\n",
      "[42]\ttrain-mlogloss:0.921674\teval-mlogloss:0.947593\n",
      "[43]\ttrain-mlogloss:0.917761\teval-mlogloss:0.944216\n",
      "[44]\ttrain-mlogloss:0.914032\teval-mlogloss:0.941015\n",
      "[45]\ttrain-mlogloss:0.910504\teval-mlogloss:0.938003\n",
      "[46]\ttrain-mlogloss:0.907231\teval-mlogloss:0.935252\n",
      "[47]\ttrain-mlogloss:0.904065\teval-mlogloss:0.93258\n",
      "[48]\ttrain-mlogloss:0.901062\teval-mlogloss:0.930073\n",
      "[49]\ttrain-mlogloss:0.898214\teval-mlogloss:0.927768\n",
      "[50]\ttrain-mlogloss:0.895445\teval-mlogloss:0.925518\n",
      "[51]\ttrain-mlogloss:0.892841\teval-mlogloss:0.923424\n",
      "[52]\ttrain-mlogloss:0.890323\teval-mlogloss:0.921392\n",
      "[53]\ttrain-mlogloss:0.887933\teval-mlogloss:0.919576\n",
      "[54]\ttrain-mlogloss:0.885666\teval-mlogloss:0.917857\n",
      "[55]\ttrain-mlogloss:0.883508\teval-mlogloss:0.91618\n",
      "[56]\ttrain-mlogloss:0.881409\teval-mlogloss:0.914535\n",
      "[57]\ttrain-mlogloss:0.87946\teval-mlogloss:0.913057\n",
      "[58]\ttrain-mlogloss:0.87756\teval-mlogloss:0.911627\n",
      "[59]\ttrain-mlogloss:0.875806\teval-mlogloss:0.910333\n",
      "[60]\ttrain-mlogloss:0.874014\teval-mlogloss:0.909051\n",
      "[61]\ttrain-mlogloss:0.872294\teval-mlogloss:0.907857\n",
      "[62]\ttrain-mlogloss:0.870657\teval-mlogloss:0.906689\n",
      "[63]\ttrain-mlogloss:0.869206\teval-mlogloss:0.905659\n",
      "[64]\ttrain-mlogloss:0.867748\teval-mlogloss:0.904644\n",
      "[65]\ttrain-mlogloss:0.86626\teval-mlogloss:0.90364\n",
      "[66]\ttrain-mlogloss:0.864811\teval-mlogloss:0.902717\n",
      "[67]\ttrain-mlogloss:0.863466\teval-mlogloss:0.901843\n",
      "[68]\ttrain-mlogloss:0.862196\teval-mlogloss:0.901036\n",
      "[69]\ttrain-mlogloss:0.86089\teval-mlogloss:0.900214\n",
      "[70]\ttrain-mlogloss:0.859534\teval-mlogloss:0.899369\n",
      "[71]\ttrain-mlogloss:0.858335\teval-mlogloss:0.898652\n",
      "[72]\ttrain-mlogloss:0.857131\teval-mlogloss:0.897943\n",
      "[73]\ttrain-mlogloss:0.856\teval-mlogloss:0.897296\n",
      "[74]\ttrain-mlogloss:0.854941\teval-mlogloss:0.896693\n",
      "[75]\ttrain-mlogloss:0.853877\teval-mlogloss:0.896128\n",
      "[76]\ttrain-mlogloss:0.852798\teval-mlogloss:0.895536\n",
      "[77]\ttrain-mlogloss:0.851735\teval-mlogloss:0.894996\n",
      "[78]\ttrain-mlogloss:0.850776\teval-mlogloss:0.894478\n",
      "[79]\ttrain-mlogloss:0.849838\teval-mlogloss:0.893981\n",
      "[80]\ttrain-mlogloss:0.848977\teval-mlogloss:0.893548\n",
      "[81]\ttrain-mlogloss:0.848048\teval-mlogloss:0.893115\n",
      "[82]\ttrain-mlogloss:0.847216\teval-mlogloss:0.892708\n",
      "[83]\ttrain-mlogloss:0.846303\teval-mlogloss:0.892284\n",
      "[84]\ttrain-mlogloss:0.845445\teval-mlogloss:0.891942\n",
      "[85]\ttrain-mlogloss:0.844586\teval-mlogloss:0.891541\n",
      "[86]\ttrain-mlogloss:0.843782\teval-mlogloss:0.89119\n",
      "[87]\ttrain-mlogloss:0.842997\teval-mlogloss:0.890886\n",
      "[88]\ttrain-mlogloss:0.842203\teval-mlogloss:0.890577\n",
      "[89]\ttrain-mlogloss:0.841554\teval-mlogloss:0.890319\n",
      "[90]\ttrain-mlogloss:0.840863\teval-mlogloss:0.890031\n",
      "[91]\ttrain-mlogloss:0.840173\teval-mlogloss:0.889758\n",
      "[92]\ttrain-mlogloss:0.839429\teval-mlogloss:0.889499\n",
      "[93]\ttrain-mlogloss:0.838708\teval-mlogloss:0.889242\n",
      "[94]\ttrain-mlogloss:0.838036\teval-mlogloss:0.889026\n",
      "[95]\ttrain-mlogloss:0.837319\teval-mlogloss:0.888744\n",
      "[96]\ttrain-mlogloss:0.836693\teval-mlogloss:0.888539\n",
      "[97]\ttrain-mlogloss:0.836088\teval-mlogloss:0.888377\n",
      "[98]\ttrain-mlogloss:0.835503\teval-mlogloss:0.888189\n",
      "[99]\ttrain-mlogloss:0.834895\teval-mlogloss:0.888023\n",
      "[100]\ttrain-mlogloss:0.834297\teval-mlogloss:0.887896\n",
      "[101]\ttrain-mlogloss:0.833786\teval-mlogloss:0.887728\n",
      "[102]\ttrain-mlogloss:0.833125\teval-mlogloss:0.887524\n",
      "[103]\ttrain-mlogloss:0.832469\teval-mlogloss:0.887348\n",
      "[104]\ttrain-mlogloss:0.831853\teval-mlogloss:0.887154\n",
      "[105]\ttrain-mlogloss:0.831218\teval-mlogloss:0.887007\n",
      "[106]\ttrain-mlogloss:0.830728\teval-mlogloss:0.886913\n",
      "[107]\ttrain-mlogloss:0.830073\teval-mlogloss:0.886791\n",
      "[108]\ttrain-mlogloss:0.829552\teval-mlogloss:0.886678\n",
      "[109]\ttrain-mlogloss:0.828952\teval-mlogloss:0.886556\n",
      "[110]\ttrain-mlogloss:0.828349\teval-mlogloss:0.886402\n",
      "[111]\ttrain-mlogloss:0.827783\teval-mlogloss:0.886316\n",
      "[112]\ttrain-mlogloss:0.827184\teval-mlogloss:0.88628\n",
      "[113]\ttrain-mlogloss:0.826526\teval-mlogloss:0.886146\n",
      "[114]\ttrain-mlogloss:0.826038\teval-mlogloss:0.886048\n",
      "[115]\ttrain-mlogloss:0.825382\teval-mlogloss:0.885933\n",
      "[116]\ttrain-mlogloss:0.824791\teval-mlogloss:0.885844\n",
      "[117]\ttrain-mlogloss:0.824176\teval-mlogloss:0.885777\n",
      "[118]\ttrain-mlogloss:0.82349\teval-mlogloss:0.88567\n",
      "[119]\ttrain-mlogloss:0.822841\teval-mlogloss:0.885628\n",
      "[120]\ttrain-mlogloss:0.822291\teval-mlogloss:0.885576\n",
      "[121]\ttrain-mlogloss:0.821667\teval-mlogloss:0.885525\n",
      "[122]\ttrain-mlogloss:0.821018\teval-mlogloss:0.885488\n",
      "[123]\ttrain-mlogloss:0.82032\teval-mlogloss:0.885393\n",
      "[124]\ttrain-mlogloss:0.819712\teval-mlogloss:0.885351\n",
      "[125]\ttrain-mlogloss:0.819056\teval-mlogloss:0.885292\n",
      "[126]\ttrain-mlogloss:0.818478\teval-mlogloss:0.885241\n",
      "[127]\ttrain-mlogloss:0.817882\teval-mlogloss:0.885224\n",
      "[128]\ttrain-mlogloss:0.817303\teval-mlogloss:0.885273\n",
      "[129]\ttrain-mlogloss:0.816668\teval-mlogloss:0.885263\n",
      "[130]\ttrain-mlogloss:0.816172\teval-mlogloss:0.885294\n",
      "[131]\ttrain-mlogloss:0.815462\teval-mlogloss:0.885255\n",
      "[132]\ttrain-mlogloss:0.814871\teval-mlogloss:0.885294\n",
      "[133]\ttrain-mlogloss:0.814256\teval-mlogloss:0.885339\n",
      "[134]\ttrain-mlogloss:0.813687\teval-mlogloss:0.885296\n",
      "[135]\ttrain-mlogloss:0.813048\teval-mlogloss:0.885286\n",
      "[136]\ttrain-mlogloss:0.812489\teval-mlogloss:0.885275\n",
      "[137]\ttrain-mlogloss:0.811974\teval-mlogloss:0.885287\n",
      "[138]\ttrain-mlogloss:0.811358\teval-mlogloss:0.885244\n",
      "[139]\ttrain-mlogloss:0.810756\teval-mlogloss:0.885242\n",
      "[140]\ttrain-mlogloss:0.810285\teval-mlogloss:0.885234\n",
      "[141]\ttrain-mlogloss:0.809596\teval-mlogloss:0.885222\n",
      "[142]\ttrain-mlogloss:0.808958\teval-mlogloss:0.885195\n",
      "[143]\ttrain-mlogloss:0.808331\teval-mlogloss:0.885226\n",
      "[144]\ttrain-mlogloss:0.8077\teval-mlogloss:0.885265\n",
      "[145]\ttrain-mlogloss:0.807113\teval-mlogloss:0.885264\n",
      "[146]\ttrain-mlogloss:0.806627\teval-mlogloss:0.885303\n",
      "[147]\ttrain-mlogloss:0.806062\teval-mlogloss:0.885299\n",
      "[148]\ttrain-mlogloss:0.805524\teval-mlogloss:0.885315\n",
      "[149]\ttrain-mlogloss:0.804971\teval-mlogloss:0.885338\n",
      "[150]\ttrain-mlogloss:0.804342\teval-mlogloss:0.885326\n"
     ]
    }
   ],
   "source": [
    "train_predict(all_df, features, prod_features, \"2016-05-28\", cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict(all_df, features, prod_features, \"2016-06-28\", cv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_python_linux)",
   "language": "python",
   "name": "ml_python_linux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
